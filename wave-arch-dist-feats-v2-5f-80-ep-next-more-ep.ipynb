{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n# os.system('pip install pytorch_toolbelt')\nimport pandas as pd\nimport numpy as np\nimport json\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000\n# import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score\nimport time\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n# from sklearn.model_selection import KFold\nimport gc\n\nfrom tqdm import tqdm\nfrom itertools import groupby, accumulate\nfrom random import shuffle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut\nfrom sklearn.preprocessing import MinMaxScaler\n# from pytorch_toolbelt import losses as L","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_val=100\n# numpy RNG\nnp.random.seed(seed_val)\n\n# pytorch RNGs\ntorch.manual_seed(seed_val)\ntorch.backends.cudnn.deterministic = True\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(seed_val)\n\n# python RNG\nimport random\nrandom.seed(seed_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ss = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\ntrain = pd.read_csv('/kaggle/input/data-without-drift/train_clean.csv')\ntest = pd.read_csv('../input/data-without-drift/test_clean.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5)); res = 1000\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model association"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.index<500000*2,'model']='1s'\ntrain.loc[((train.index>=500000*2)&(train.index<500000*3))|((train.index>=500000*6)&(train.index<500000*7)),'model']='1f'\ntrain.loc[((train.index>=500000*3)&(train.index<500000*4))|((train.index>=500000*7)&(train.index<500000*8)),'model']='3'\ntrain.loc[((train.index>=500000*5)&(train.index<500000*6))|((train.index>=500000*8)&(train.index<500000*9)),'model']='5'\ntrain.loc[((train.index>=500000*4)&(train.index<500000*5))|((train.index>=500000*9)&(train.index<500000*10)),'model']='10'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.model.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[((test.index>=100000*0)&(test.index<100000*1))|((test.index>=100000*3)&(test.index<100000*4)),'model']='1s'\ntest.loc[((test.index>=100000*8)&(test.index<100000*9))|((test.index>=100000*10)&(test.index<100000*20)),'model']='1s'\ntest.loc[((test.index>=100000*4)&(test.index<100000*5)),'model']='1f'\ntest.loc[((test.index>=100000*1)&(test.index<100000*2))|((test.index>=100000*9)&(test.index<100000*10)),'model']='3'\ntest.loc[((test.index>=100000*2)&(test.index<100000*3))|((test.index>=100000*6)&(test.index<100000*7)),'model']='5'\ntest.loc[((test.index>=100000*5)&(test.index<100000*6))|((test.index>=100000*7)&(test.index<100000*8)),'model']='10'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.model.value_counts(dropna=False)\n# train.groupby(['model','open_channels']).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=train.groupby(['model','open_channels']).signal.agg(['mean','std']).reset_index()\na.sort_values(['model','open_channels'],inplace=True)\na['mean_diff']=a['mean']-a['mean'].shift(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a.loc[a.model=='10','std']=0.413\na.loc[a.model=='1f','std']=0.2479\na.loc[a.model=='1s','std']=0.25\na.loc[a.model=='3','std']=0.507\na.loc[a.model=='5','std']=0.2909\n\n# -4.193538-1.2336295466666667\na.loc[(a.model=='10')&(a.open_channels==0),'mean']=-5.4271675466666665\n\na['mean_diff']=a['mean']-a['mean'].shift(1)\na.loc[a.open_channels==0,'mean_diff']=np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a['mean_diff'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pulse_id']=((train.index//500000)+1).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['model','pulse_id','open_channels']).signal.agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for mod in ['10', '1f', '1s', '3', '5']:\n    for chann in train.loc[train.model==mod,'open_channels'].unique():\n        train.loc[train.model==mod , \n                  str(chann)+'_zscore']=(train.loc[train.model==mod ,\n                                                   'signal']-a.loc[((a.model==mod)&(a.open_channels==chann)),\n                                                                   'mean'].unique()[0])/(a.loc[(a.model==mod)&(a.open_channels==chann),'std'].unique()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for mod in ['10', '1f', '1s', '3', '5']:\n    for chann in train.loc[train.model==mod,'open_channels'].unique():\n        test.loc[test.model==mod , \n                  str(chann)+'_zscore']=(test.loc[test.model==mod ,\n                                                   'signal']-a.loc[((a.model==mod)&(a.open_channels==chann)),\n                                                                   'mean'].unique()[0])/(a.loc[(a.model==mod)&(a.open_channels==chann),'std'].unique()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zscore_cols=[str(i) +'_zscore' for i in range(11)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[zscore_cols]=train[zscore_cols].abs()\ntest[zscore_cols]=test[zscore_cols].abs()\ntrain=train.fillna(1000)\ntest=test.fillna(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def using_indexed_assignment(x,n_):\n    \"https://stackoverflow.com/a/5284703/190597 (Sven Marnach)\"\n    result = np.empty(len(x), dtype=int)\n    temp = x\n    result[temp] = np.arange(len(x))\n    result= list(result)\n    return result.index(n_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['dist_target']=[using_indexed_assignment(i,0) for i in np.argsort(train[zscore_cols].values)]\ntest['dist_target']=[using_indexed_assignment(i,0) for i in np.argsort(test[zscore_cols].values)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['dist_target_2']=[using_indexed_assignment(i,1) for i in np.argsort(train[zscore_cols].values)]\ntest['dist_target_2']=[using_indexed_assignment(i,1) for i in np.argsort(test[zscore_cols].values)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(train.open_channels,train.dist_target,average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train['model'].unique():\n    print(i,f1_score(train[train.model==i].open_channels,train[train.model==i].dist_target,average='macro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.model=='10'].open_channels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a1=train[(train.model=='10')&(train.pulse_id==5)].open_channels.values\na2=train[(train.model=='10')&(train.pulse_id==10)].open_channels.values\nlen(a1),len(a2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transition_matrix(transitions):\n    n = 1+ max(transitions) #number of states\n\n    M = [[0]*n for _ in range(n)]\n\n    for (i,j) in zip(transitions,transitions[1:]):\n        M[i][j] += 1\n\n    #now convert to probabilities:\n    for row in M:\n        s = sum(row)\n        if s > 0:\n            row[:] = [f/s for f in row]\n    return M\n\n#test:\n\n# t = [1,1,2,6,8,5,5,7,8,8,1,1,4,5,5,0,0,0,1,1,4,4,5,1,3,3,4,5,4,1,1]\n# m = transition_matrix(t)\n# for row in m: print(' '.join('{0:.2f}'.format(x) for x in row))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1=transition_matrix(a1)\nm2=transition_matrix(a2)\nm=np.mean([m1,m2],axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# m10=train[train.model=='10']\n# e=m10[m10.dist_target!=m10.open_channels][['open_channels','dist_target']]\n# e['comb_']=e.open_channels.astype('str')+'_'+e.dist_target.astype('str')\n# e['comb_'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# m10=train[train.model=='10']\n# e=m10[m10.dist_target!=m10.open_channels]\n# e['delta_']=e.apply(lambda x :x[str(x['dist_target_2'])+'_zscore']-x[str(x['dist_target'])+'_zscore'],axis=1)\n# e['delta_'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# m10=train[train.model=='10']\n# e=m10[m10.dist_target==m10.open_channels]\n# e['delta_']=e.apply(lambda x :x[str(x['dist_target_2'])+'_zscore']-x[str(x['dist_target'])+'_zscore'],axis=1)\n# e['delta_'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['dist_target_col']=train.dist_target.astype('str')+'_zscore'\ntrain['dist_target_2_col']=train.dist_target_2.astype('str')+'_zscore'\ntrain['delta_']=train.apply(lambda x :x[x['dist_target_2_col']]-x[x['dist_target_col']],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['dist_target_col']=test.dist_target.astype('str')+'_zscore'\ntest['dist_target_2_col']=test.dist_target_2.astype('str')+'_zscore'\ntest['delta_']=test.apply(lambda x :x[x['dist_target_2_col']]-x[x['dist_target_col']],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q=0.8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train[train.delta_<=q].dist_target==train[train.delta_<=q].open_channels).value_counts(1),f1_score(train[train.delta_<=q].open_channels,\n                                                                                                   train[train.delta_<=q].dist_target,average='macro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### on train"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport copy\nidx_train=pd.Series([m for j in [[i,i-1,i+1] for i in train[train.delta_<=q].index] for m in j]).sort_values().unique()\ne=train.iloc[idx_train,:]\ne['dist_target_last']=e['dist_target'].shift(1)\ne['dist_target_next']=e['dist_target'].shift(-1)\ne=e.fillna(0)\nfor i in ['dist_target','dist_target_2','dist_target_last','dist_target_next']:\n    e[i]=e[i].astype('int')\n# e.shape,len(idx_train),train[train.delta_<=q].shape\ne['prob_1n']=e.apply(lambda x: m[x['dist_target'],x['dist_target_next']],axis=1)\ne['prob_2n']=e.apply(lambda x: m[x['dist_target_2'],x['dist_target_next']],axis=1)\ne['prob_l1']=e.apply(lambda x: m[x['dist_target_last'],x['dist_target']],axis=1)\ne['prob_l2']=e.apply(lambda x: m[x['dist_target_last'],x['dist_target_2']],axis=1)\ne['prob_1']=e.prob_1n*e.prob_l1\ne['prob_2']=e.prob_2n*e.prob_l2\ne['is_greater_12']=(e['prob_1']>e['prob_2']).astype('int')\ne['dist_dummy']=copy.deepcopy(e.dist_target)\ne.loc[((e['is_greater_12']==0)&(e.delta_<=q)),'dist_dummy']=e.loc[((e['is_greater_12']==0)&(e.delta_<=q)),\n                                                                  'dist_target_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(e.open_channels,e.dist_target,average='macro'),f1_score(e.open_channels,e.dist_dummy,average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.merge(e[['time','dist_dummy']],on='time',how='left')\ntrain.loc[((train['dist_dummy'].notnull())&(train.model.isin(['10','5']))),\n          'dist_target']=train.loc[((train['dist_dummy'].notnull())&(train.model.isin(['10','5']))),'dist_dummy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(train.open_channels,train.dist_target,average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train['model'].unique():\n    print(i,f1_score(train[train.model==i].open_channels,train[train.model==i].dist_target,average='macro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### on test"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport copy\nidx_test=pd.Series([m for j in [[i,i-1,i+1] for i in test[test.delta_<=q].index] for m in j]).sort_values().unique()\ne=test.iloc[idx_test,:]\ne['dist_target_last']=e['dist_target'].shift(1)\ne['dist_target_next']=e['dist_target'].shift(-1)\ne=e.fillna(0)\nfor i in ['dist_target','dist_target_2','dist_target_last','dist_target_next']:\n    e[i]=e[i].astype('int')\nprint(e.shape,len(idx_test),test[test.delta_<=q].shape)\ne['prob_1n']=e.apply(lambda x: m[x['dist_target'],x['dist_target_next']],axis=1)\ne['prob_2n']=e.apply(lambda x: m[x['dist_target_2'],x['dist_target_next']],axis=1)\ne['prob_l1']=e.apply(lambda x: m[x['dist_target_last'],x['dist_target']],axis=1)\ne['prob_l2']=e.apply(lambda x: m[x['dist_target_last'],x['dist_target_2']],axis=1)\ne['prob_1']=e.prob_1n*e.prob_l1\ne['prob_2']=e.prob_2n*e.prob_l2\ne['is_greater_12']=(e['prob_1']>e['prob_2']).astype('int')\ne['dist_dummy']=copy.deepcopy(e.dist_target)\ne.loc[((e['is_greater_12']==0)&(e.delta_<=q)),'dist_dummy']=e.loc[((e['is_greater_12']==0)&(e.delta_<=q)),\n                                                                  'dist_target_2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(e.dist_target==e.dist_dummy).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=test.merge(e[['time','dist_dummy']],on='time',how='left')\ntest.loc[((test['dist_dummy'].notnull())&(test.model.isin(['10','5']))),\n          'dist_target']=test.loc[((test['dist_dummy'].notnull())&(test.model.isin(['10','5']))),'dist_dummy']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"group_size=1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['group'] = train.groupby(train.index//group_size, sort=False)['signal'].agg(['ngroup']).values\nfolds=GroupKFold(n_splits=5)\nfolds=[(tr,val) for tr,val in folds.split(train,train['open_channels'],train['group'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[(len(i[0]),len(i[1])) for i in folds]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EarlyStopping(object):\n    def __init__(self,patience,higher_is_better,tolerance,save_path,model):\n        self.patience=patience\n        self.higher_is_better = higher_is_better\n        self.best_score= -100000 if self.higher_is_better else 100000\n        self.tolerance=tolerance ### score should inc/dec by this much to by pass early stopping\n        self.ctr=0\n        self.save_path=save_path\n        self.model=model\n\n    def check(self,metric):\n        if self.ctr<self.patience:\n\n            if self.higher_is_better:\n                if metric>=(self.best_score+self.tolerance):\n                    self.best_score=metric\n                    self.ctr =0\n                    torch.save(self.model.state_dict(), self.save_path)\n                else:\n                     self.ctr += 1\n            else:\n                if metric<=(self.best_score-self.tolerance):\n                    self.best_score=metric\n                    self.ctr =0\n                    torch.save(self.model.state_dict(), self.save_path)\n                else:\n                     self.ctr += 1\n            return True\n\n        else:\n            return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_=[ 'signal', \n#         'model',\n#        '10_zscore', '9_zscore',\n#        '8_zscore', '7_zscore', '6_zscore', '5_zscore', '4_zscore', '3_zscore',\n#        '2_zscore', '1_zscore', '0_zscore', \n       'dist_target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cols_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a,b=next(iter(train_dataloader))\n# a.shape,b.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class IonDataset(Dataset):\n\n    def __init__(self,X,y,seq_len=1000):\n\n        self.seq_len=seq_len\n        self.size_=int(X.shape[0]/seq_len)\n        self.x=X.astype(np.float32).values\n\n        if y is not None:\n            self.y=y.values\n        else:\n            self.y=None\n\n    def __len__(self):\n        return self.size_\n\n    def __getitem__(self,idx):\n        if self.y is not None:\n            return  self.x[idx*self.seq_len:(idx+1)*self.seq_len],\\\n                    self.y[idx*self.seq_len:(idx+1)*self.seq_len],\\\n\n        else:\n            return self.x[idx*self.seq_len:(idx+1)*self.seq_len]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Waveblock(nn.Module):\n    def __init__(self,num_layers,kernel_size,input_channels,dilation_dim,skip_conv_dim,res_conv_dim,output_channels):\n        super(Waveblock,self).__init__()\n        ###res conv dim and causal conv dim will be same so that they can be added in the residual block\n        self.causal_conv=nn.Conv1d(input_channels,res_conv_dim,kernel_size=1)\n        \n        self.gate_convs=nn.ModuleList()\n        self.filter_convs=nn.ModuleList()\n        ##res scaling and skip scaling convs should not affect size of feature map\n        self.res_scaling_convs=nn.ModuleList()\n        self.skip_scaling_convs=nn.ModuleList()\n        \n        self.num_layers=num_layers\n        \n        for i in range(self.num_layers):\n            dilation=2**i\n            ##so that Lin==Lout\n            pad_size=int(dilation*(kernel_size-1)/2)\n            \n            self.gate_convs.append(nn.Conv1d(res_conv_dim,dilation_dim,dilation=dilation,kernel_size=kernel_size,padding=pad_size))\n            self.filter_convs.append(nn.Conv1d(res_conv_dim,dilation_dim,dilation=dilation,kernel_size=kernel_size,padding=pad_size))\n            self.res_scaling_convs.append(nn.Conv1d(dilation_dim,res_conv_dim,kernel_size=1))\n            self.skip_scaling_convs.append(nn.Conv1d(dilation_dim,skip_conv_dim,kernel_size=1))\n        \n        self.final_conv1=nn.Conv1d(skip_conv_dim,skip_conv_dim,kernel_size=1)\n        self.final_conv2=nn.Conv1d(skip_conv_dim,output_channels,kernel_size=1)\n        \n    def forward(self,x):\n        ###shape should be (batch,input channels,seq_len)\n        x=self.causal_conv(x)\n        \n        for i in range(self.num_layers):\n            #print('residual block:',i)\n            filter_=self.filter_convs[i](x)\n            gate_=self.gate_convs[i](x)\n            ##element wise multiplication\n            gated_act_=(nn.Tanh()(filter_))*(nn.Sigmoid()(gate_))\n            #print(gated_act_.shape)\n            #print(self.res_scaling_convs[i](gated_act_).shape,x.shape)\n            x=torch.add(self.res_scaling_convs[i](gated_act_),x)\n            ##this x will feed into next layer(residual block)\n            \n            skip_output=self.skip_scaling_convs[i](gated_act_)\n            if i==0:\n                final_output=skip_output\n            else:\n                final_output=torch.add(final_output,skip_output)\n        \n        final_output=nn.ReLU()(final_output)\n        final_output=self.final_conv1(final_output)\n        final_output=nn.ReLU()(final_output)\n        final_output=self.final_conv2(final_output)\n        \n        ##final output shape will be (batch,output_channels,seq_len)\n        ##for LSTM we need (batch,seq_len,input_dim),permute later to (0,2,1)\n        return final_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self,input_size,hidden_size,output_size,num_rnn_layers,hidden_layers,bidirectional,dropout):\n        \n        super().__init__()\n        self.input_size=input_size\n        self.hidden_size=hidden_size\n        self.output_size=output_size\n        self.num_rnn_layers=num_rnn_layers\n        self.bidirectional=bidirectional\n        self.dropout=dropout\n        self.len_hidden_layers=isinstance(hidden_layers,list)\n        \n        self.rnn=nn.GRU(batch_first=True,input_size=self.input_size,hidden_size=self.hidden_size,bidirectional=self.bidirectional,\n                         num_layers=self.num_rnn_layers,dropout=self.dropout)\n        \n        #Input layer\n        \n        if hidden_layers:\n            first_layer=nn.Linear(self.hidden_size*2 if self.bidirectional else self.hidden_size,hidden_layers[0])\n        \n        #Hidden layers\n        if self.len_hidden_layers:\n            self.hidden_layers=nn.ModuleList( [first_layer] + [nn.Linear(hidden_layers[i],hidden_layers[i+1]) for i in range(len(hidden_layers)-1)])\n            for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data)\n        else:\n            self.hidden_layers=first_layer\n            nn.init.kaiming_normal_(self.hidden_layers.weight.data)\n            \n        #Output layer\n        \n        self.output_layer=nn.Linear(hidden_layers[-1],self.output_size)\n        nn.init.kaiming_normal_(self.output_layer.weight.data)\n        \n        self.activation_fn=torch.relu\n        self.dropout=nn.Dropout(dropout)\n    \n    def forward(self,x):\n#         print(x.shape)\n        \n#         batch_size=x.size(0)\n        \n        #print('input before permute shape: ',x.shape)\n        \n        ### use permute when input dimension more than 1\n        \n#         x=x.view(x.shape[0],x.shape[-1],1)\n        \n        #print('input to model shape: ',x.shape)\n        \n        ###shape should be (batch,seq_en,input_dim)\n        \n        outputs,hidden=self.rnn(x)\n        \n#         print(outputs.shape)\n        x=self.dropout(self.activation_fn(outputs))\n        \n        if self.len_hidden_layers:\n            for layer in self.hidden_layers:\n                x = self.activation_fn(layer(x))\n                x = self.dropout(x)\n        else:\n            x = self.activation_fn(self.hidden_layers(x))\n            x = self.dropout(x)\n        \n        x=self.output_layer(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wavenet=Waveblock(num_layers=10,kernel_size=2,input_channels=1,dilation_dim=128,\n#                       skip_conv_dim=128,\n#                       res_conv_dim=24,\n#                       output_channels=3)\n# rnn = RNN(input_size=3,hidden_size=128,output_size=11,num_rnn_layers=3,\n#           hidden_layers=[512,64],bidirectional=True,dropout=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class model_(nn.Module):\n    def __init__(self,wavenet_dict,rnn_dict):\n        super().__init__()\n        self.waveblock=Waveblock(**wavenet_dict)\n        self.rnn=RNN(**rnn_dict)\n    def forward(self,x):\n        ###shape should be (batch,input channels,seq_len)\n        #print('before permute shape:',x.shape)\n        #x=x.view(x.shape[0],1,x.shape[-1])\n        x=x.permute(0,2,1)\n        #print('before model shape:',x.shape)\n        x=self.waveblock(x)\n        #print('after waveblock shape:',x.shape)\n        x=x.permute(0,2,1)\n        ###shape should be (batch,seq_len,input dim)\n        #print('before rnn shape:',x.shape)\n        x=self.rnn(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(folds),len(folds[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_state_dict():\n    return {'model_dict':None,'epoch_num':0,'batch_num':0,'train_params':{'avg_loss':[],'metric':[]},\n            'valid_params':{'avg_loss':[],'metric':[]}}\n\ndef get_F1_score(true_list,preds_list):\n    return f1_score(np.concatenate(true_list),np.concatenate(preds_list),average='macro')\n\ndef validate_loop(valid_dl,model,loss_fn,model_dict,beta=0.98):\n\n    model.eval()\n    with torch.no_grad():\n        loss_=0\n        pred_list=[]\n        tar_list=[]\n    for i,(num,tar) in enumerate(valid_dl):\n            num = num.to(device)\n            tar = tar.to(device)\n            preds = model(num)\n            preds=preds.view(-1,preds.shape[-1])\n            tar=tar.view(-1)\n            loss = loss_fn(preds, tar)\n            loss_ += loss.item()\n            pred_list.append(preds.cpu().detach().numpy().argmax(1))\n            tar_list.append(tar.cpu().detach().numpy())\n        \n    model_dict['valid_params']['avg_loss'].append(loss_/i)\n    model_dict['valid_params']['metric'].append(get_F1_score(tar_list,pred_list))\n\n    return model_dict\n\ndef train_loop(model,train_dl,valid_dl,num_epoch,optimizer,loss_fn,schedular,early_stopping,log_file,beta=0.98):\n\n    model_dict=init_state_dict()\n\n    for j,epoch in enumerate(range(num_epoch)):\n        st=time.time()\n\n        model.train()\n\n        loss_=0\n        pred_list=[]\n        tar_list=[]\n\n        for i, (num,tar) in enumerate(train_dl):\n\n            num = num.to(device)\n            tar = tar.to(device)\n            optimizer.zero_grad()\n            \n            preds = model(num)\n            preds=preds.view(-1,preds.shape[-1])\n            tar=tar.view(-1)\n            loss = loss_fn(preds, tar)\n            loss.backward()\n            optimizer.step()\n            \n            if schedular is not None:\n                schedular.step()\n            loss_ += loss.item()\n            model_dict['batch_num']+=1\n            \n            pred_list.append(preds.cpu().detach().numpy().argmax(1))\n            tar_list.append(tar.cpu().detach().numpy())\n\n        model_dict['train_params']['avg_loss'].append(loss_/i)\n        model_dict['train_params']['metric'].append(get_F1_score(tar_list,pred_list))\n\n        model_dict['epoch_num']=j\n\n        model_dict=validate_loop(valid_dl=valid_dl,model=model,loss_fn=loss_fn,model_dict=model_dict,beta=beta)\n\n        val_metric=model_dict['valid_params']['metric'][j]\n        \n#         if schedular is not None:\n#             schedular.step(metrics = val_metric)\n            \n        et=time.time()\n        if not early_stopping.check(val_metric):\n            return model_dict\n        \n        mssg = 'Epoch {}: train_loss:{} valid_loss:{} train_metric:{} valid_metric:{} time:{}'.format(model_dict['epoch_num'],\n                                                    round(model_dict['train_params']['avg_loss'][j],4),\n                                                    round(model_dict['valid_params']['avg_loss'][j],4),\n                                                    round(model_dict['train_params']['metric'][j],4),\n                                                    round(model_dict['valid_params']['metric'][j],4),round((et-st)/60,2))\n        \n#         slack_loggger('GTX1660 LSTM LIVERPOOL: ',mssg)\n        print (mssg)\n#         f= open(log_file, 'a')  \n#         f.write(mssg)\n#         f.close()\n\n        # model_dict['model_dict']=model.state_dict() ### keep updating the model dict if it doesn't meet early stopping\n    return model_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEQ_LEN = 1000\nIS_PRETRAINED = False\nNUM_WORKERS = 4\nbatchsize=32\ndevice=torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['open_channels']=train['open_channels'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wavenet_dict={'num_layers':8,'kernel_size':3,'input_channels':2,'dilation_dim':32,\n              'skip_conv_dim':32,'res_conv_dim':32,'output_channels':32}\nrnn_dict={'input_size':32,'hidden_size':64,'output_size':11,'num_rnn_layers':2,\n          'hidden_layers':[128],'bidirectional':True,'dropout':0.2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IS_PRETRAINED = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"### pretrained score check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def validate_loop_pretrained(valid_dl,model,loss_fn,beta=0.98):\n\n#     model.eval()\n#     with torch.no_grad():\n#         loss_=0\n#         pred_list=[]\n#         tar_list=[]\n#     for i,(num,tar) in enumerate(valid_dl):\n#             num = num.to(device)\n#             tar = tar.to(device)\n#             preds = model(num)\n#             preds=preds.view(-1,preds.shape[-1])\n#             tar=tar.view(-1)\n#             loss = loss_fn(preds, tar)\n#             loss_ += loss.item()\n#             pred_list.append(preds.cpu().detach().numpy().argmax(1))\n#             tar_list.append(tar.cpu().detach().numpy())\n        \n#     print('loss',loss_/i,'metric',get_F1_score(tar_list,pred_list))\n# #     model_dict['valid_params']['metric'].append()\n#     return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(test_dl,model,model_path=None):\n\n    if model_path is not None:\n        model.load_state_dict(torch.load(model_path))\n\n    model.eval()\n\n    with torch.no_grad():\n        pred_list=[]\n        id_list=[]\n    for i,num in enumerate(test_dl):\n            num = num.to(device)\n#             id_ = id_.to(device)\n#             id_=id_.view(-1)\n            preds=model(num)\n            preds=preds.view(-1,preds.shape[-1])\n            pred_list.append(preds.cpu().detach().numpy())\n#             print(preds.shape)\n#             id_list.append(id_.cpu().detach().numpy())\n\n#     out_ =np.concatenate(pred_list),np.concatenate(id_list)\n#     out_ =np.concatenate(pred_list)    \n    \n    return np.vstack(pred_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IS_PRETRAINED=True\n# loss_fn=nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df = IonDataset(X=test[cols_],y=None\n#                      ,seq_len=SEQ_LEN)\n# test_dl =DataLoader(test_df, batchsize, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n# # \n# preds = []\n# for i in range(3):\n      \n# #     train_df = IonDataset(X=train[cols_].iloc[folds[i][0]],\n# #                        seq_len=SEQ_LEN,y=train['open_channels'].iloc[folds[i][0]])\n# #     train_dl = DataLoader(dataset=train_df,batch_size=batchsize,shuffle=True,drop_last=True,num_workers=NUM_WORKERS)\n\n#     val_df = IonDataset(X=train[cols_].iloc[folds[i][1]],\n#                        seq_len=SEQ_LEN,y=train['open_channels'].iloc[folds[i][1]])\n#     val_dl = DataLoader(dataset=val_df,batch_size=batchsize,shuffle=True,num_workers=NUM_WORKERS)\n\n#     rnn=model_(wavenet_dict,rnn_dict).to(device)    \n#     if IS_PRETRAINED:\n#         rnn.load_state_dict(torch.load('../input/ion-switchingmodels-v2/GRU_focal_folds_{}.path'.format(i)))\n    \n#     validate_loop_pretrained(val_dl,rnn,loss_fn,beta=0.98)\n#     rnn=model_(wavenet_dict,rnn_dict).to(device)    \n#     preds.append(predict(test_dl,rnn,'../input/ion-switchingmodels-v2/GRU_focal_folds_{}.path'.format(i)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean([0.940227802425148,0.9407934877974964,0.939087731720544])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"trusted":true},"cell_type":"code","source":"IS_PRETRAINED=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(folds)):\n      \n    train_df = IonDataset(X=train[cols_].iloc[folds[i][0]],\n                       seq_len=SEQ_LEN,y=train['open_channels'].iloc[folds[i][0]])\n    train_dl = DataLoader(dataset=train_df,batch_size=batchsize,shuffle=True,drop_last=True,num_workers=NUM_WORKERS)\n\n    val_df = IonDataset(X=train[cols_].iloc[folds[i][1]],\n                       seq_len=SEQ_LEN,y=train['open_channels'].iloc[folds[i][1]])\n    val_dl = DataLoader(dataset=val_df,batch_size=batchsize,shuffle=True,num_workers=NUM_WORKERS)\n\n    rnn=model_(wavenet_dict,rnn_dict).to(device)\n    if IS_PRETRAINED:\n        rnn.load_state_dict(torch.load('../input/ionmodels5f-90ep/GRU_focal_folds_{}.path'.format(i)))\n\n    max_lr=0.001\n    early_stopping_rounds=40\n    maximize=True\n    num_epochs=50\n    loss_fn=nn.CrossEntropyLoss()\n#     loss_fn=FocalLoss(gamma=1)\n    optimizer = torch.optim.AdamW(params=rnn.parameters(),lr=max_lr)\n\n    schedular=torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,max_lr=max_lr,epochs=num_epochs,\n                                            steps_per_epoch=len(train_dl))\n    \n    es=EarlyStopping(patience=early_stopping_rounds,higher_is_better=1,tolerance=0.0001,\n                     save_path='GRU_focal_folds_{}.path'.format(i),model=rnn)\n    dict_=train_loop(model=rnn,train_dl = train_dl ,valid_dl = val_dl,log_file='GRU_log.txt',\n                 num_epoch=num_epochs,optimizer=optimizer,loss_fn=loss_fn,\n                     schedular=schedular,early_stopping=es,beta=0.98)\n#     print('BEST SCORE FOR FOLD {} IS {}'.format(i,np.max(dict_['valid_params']['metric'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.mean([93.95,94.07,93.82,93.92,93.94])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(.9385+.9395+.9377+.9374+.9385)/5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean([0.9397932522542907,0.9404833125183752,0.9381518885864977,0.9388859828821762])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_df = IonDataset(X=test[cols_],y=None\n                     ,seq_len=SEQ_LEN)\ntest_dl =DataLoader(test_df, batchsize, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\npreds = []\nfor i in range(len(folds)):\n    \n    rnn=model_(wavenet_dict,rnn_dict).to(device)\n#     rnn.load_state_dict(torch.load(\"GRU_focal_folds_{}.path\".format(i)))\n    preds.append(predict(test_dl,rnn,'GRU_focal_folds_{}.path'.format(i)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['open_channels']=pd.Series(np.mean(preds,axis=0).argmax(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['open_channels'].iloc[-1000000:].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('baseline_wavenet_GRU_3_layer_bidir.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}